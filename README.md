
# LAT adversarial_robustness


A fine tuning technique over the adversarially trained models to increase further robustness



## Running the code


### Training via LAT

```
python feature_adv_training_layer11.py

```

### Latent Attack 

```
python latent_adversarial_attack.py

```

<p>
    <img src="https://github.com/conference-submission-anon/LAT_adversarial_robustness/tree/master/cifar_LA_images_8_eps/index10orig5.png" width="220" height="240" />
</p>

![Alt text](https://github.com/conference-submission-anon/LAT_adversarial_robustness/tree/master/cifar_LA_images_8_eps/index10orig5.png)

.left[

![Alt text](https://github.com/conference-submission-anon/LAT_adversarial_robustness/cifar_LA_images_8_eps/index10orig5.png)

.caption[
Dog
]

]
right[

![Alt text](https://github.com/conference-submission-anon/LAT_adversarial_robustness/cifar_LA_images_8_eps/index10adv3.png)

.caption[
Cat
]

]


.. image:: https://github.com/conference-submission-anon/LAT_adversarial_robustness/cifar_LA_images_8_eps/index10orig5.png
.. image:: https://github.com/conference-submission-anon/LAT_adversarial_robustness/cifar_LA_images_8_eps/index10adv3.png


